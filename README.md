# CS431 Project: Deep Learning - Text Summarization System

## Objective
This project is carried out for educational purposes, focusing on exploring the Transformer architecture. While we do not employ State-of-the-Art (SOTA) methods, the project aims to implement and study the fundamental techniques behind the Transformer model.

## Methodology
- **Transformer from Scratch** with pytorch: implement the Transformer model from scratch, with model hyperparameters referenced from the paper [Attention is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
- **Phobert2Phobert**: To leverage pretrained models, we use one Phobert model as the encoder and another Phobert model as the decoder. This approach helps improve text summarization by utilizing the features learned by the Phobert models.

